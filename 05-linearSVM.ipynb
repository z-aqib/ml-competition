{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbd9859",
   "metadata": {},
   "source": [
    "Date: 16th Nov 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f2a4a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10bfbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 1: Imports & CONFIG\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    MaxAbsScaler,\n",
    "    RobustScaler,\n",
    "    Normalizer,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4204ec6",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07b14155",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # --- Meta / experiment info ---\n",
    "    \"EXP_NAME\": \"experiment_35A\",  # any short name for this run\n",
    "    \"EXPERIMENTER\": \"Zuha\",              # \"Zuha\" | \"Maryam\" | \"Maham\"\n",
    "    \"MODEL_FAMILY\": \"svm\",          # \"lightgbm\" | \"xgboost\" | \"mlp_early\" | \"mlp_two_tower\"\n",
    "                                          # \"logreg\" | \"svm\" | \"rf\" | \"catboost\" | \"ensemble\"\n",
    "\n",
    "    # --- Paths (update DATA_DIR when you know Kaggle path) ---\n",
    "    # On Kaggle, you'll usually have something like: \"/kaggle/input/dss-ml-competition/\"\n",
    "    \"DATA_DIR\": \"./data\",                # folder containing train_part1.json & test.json\n",
    "    \"TRAIN_FILENAME\": \"train_p1.json\",\n",
    "    \"TEST_FILENAME\": \"test.json\",\n",
    "    \"SUBMISSION_OUTPUT_DIR\": \"./submissions\",  # where to save CSV submissions\n",
    "\n",
    "    # --- Target / ID / feature mode ---\n",
    "    \"TARGET_COLUMN\": \"label\",            # name of target in train JSON\n",
    "    \"ID_COLUMN\": \"id\",                   # for train: hashed id, for test: integer\n",
    "    \"FEATURE_MODE\": \"img+text\",          # \"img+text\" | \"img_only\" | \"text_only\"\n",
    "                                         # (later you can try ablations)\n",
    "\n",
    "    # --- Reproducibility ---\n",
    "    \"SEED\": 42,                          # any integer; try 42, 2025, 7, etc.\n",
    "    \"NP_RANDOM_SEED\": 42,                # keep same as SEED or change if you want\n",
    "\n",
    "    # --- Train / validation strategy (for local eval) ---\n",
    "    \"VALIDATION_SCHEME\": \"cv_only\",      # \"cv_only\"          → only K-fold CV\n",
    "                                         # \"holdout_only\"     → single train/val split\n",
    "                                         # \"holdout+cv\"       → both split + CV\n",
    "\n",
    "    # If using holdout (for VALIDATION_SCHEME != \"cv_only\"):\n",
    "    \"VAL_SIZE\": 0.2,                     # fraction of train data for validation (e.g. 0.1, 0.2)\n",
    "    \"VAL_STRATIFY\": True,                # True to stratify by label in train/val split\n",
    "    \"VAL_RANDOM_STATE\": 42,              # seed for train/val split\n",
    "\n",
    "    # --- Cross-validation (K-fold) ---\n",
    "    \"USE_STRATIFIED_KFOLD\": True,        # almost always True for classification\n",
    "    \"N_FOLDS\": 7,                        # typical: 3, 5, 7, 10\n",
    "    \"SHUFFLE_FOLDS\": True,               # shuffle before splitting into folds\n",
    "    \"FOLDS_RANDOM_STATE\": 42,            # seed for fold splitting\n",
    "\n",
    "    # --- Scaling / normalization ---\n",
    "    \"SCALER_TYPE\": \"minmax\",               # \"none\"     → no scaling (good for tree models)\n",
    "                                         # \"standard\" → StandardScaler (zero mean, unit variance)\n",
    "                                         # \"minmax\"   → MinMaxScaler (0–1)\n",
    "                                         # \"maxabs\"   → MaxAbsScaler ([-1, 1] for sparse-like)\n",
    "                                         # \"robust\"   → RobustScaler (robust to outliers)\n",
    "                                         # \"l2_norm\"  → normalize each sample to unit L2 norm\n",
    "\n",
    "    # --- Missing values handling (numeric) ---\n",
    "    \"MISSING_VALUE_STRATEGY\": \"mean\",    # \"none\"     → assume no NaNs; just assert & crash if found\n",
    "                                         # \"mean\"     → fill NaNs with column means\n",
    "                                         # \"median\"   → fill with column medians\n",
    "                                         # \"constant\" → fill with constant (see MISSING_FILL_VALUE)\n",
    "                                         # \"zero\"     → fill with 0.0\n",
    "\n",
    "    \"MISSING_FILL_VALUE\": 0.0,           # used when strategy == \"constant\" or \"zero\"\n",
    "\n",
    "    # --- PCA dimensionality reduction ---\n",
    "    \"USE_PCA\": True,                    # True → apply PCA after scaling (if any)\n",
    "    \"PCA_N_COMPONENTS\": 64,             # e.g. 64, 128, 256, 512; <= 1024 total features\n",
    "    \"PCA_WHITEN\": True,                 # True → decorrelate & scale to unit variance\n",
    "    \"PCA_SVD_SOLVER\": \"full\",            # \"auto\" | \"full\" | \"randomized\"\n",
    "    \"PCA_RANDOM_STATE\": 42,              # for randomized solver, etc.\n",
    "\n",
    "    # --- Class imbalance handling (general) ---\n",
    "    \"IMBALANCE_MODE\": \"balanced\",            # \"none\"           → do nothing\n",
    "                                         # \"balanced\"       → use class_weight='balanced' (for LR/SVM/MLP)\n",
    "                                         # \"scale_pos_weight\" → for tree models like XGB/LGB\n",
    "                                         # \"is_unbalance\"   → LightGBM's built-in option\n",
    "\n",
    "    # --- Threshold for converting probabilities → class labels ---\n",
    "    \"DECISION_THRESHOLD\": 0.5,           # default 0.5; you can tune (0.4–0.6, etc.)\n",
    "\n",
    "    # --- Logging / runs ---\n",
    "    \"SAVE_LOCAL_LOGS\": True,             # later: saves experiments_master.csv etc.\n",
    "    \"LOGS_DIR\": \"./logs\",                # folder for logs/CSVs if you want\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa7be3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure directories exist\n",
    "Path(CONFIG[\"SUBMISSION_OUTPUT_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"LOGS_DIR\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40997a",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1ecec",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31d2476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 2: Data Loading Utils\n",
    "# =========================\n",
    "\n",
    "def load_json_list(path):\n",
    "    \"\"\"\n",
    "    Loads a JSON file that contains either:\n",
    "    - a list of JSON objects, OR\n",
    "    - JSON lines (one object per line)\n",
    "    and returns a Python list of dicts.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "        if not text:\n",
    "            return []\n",
    "        # Try parse as a JSON array first\n",
    "        try:\n",
    "            data = json.loads(text)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            else:\n",
    "                # If it's a single object, wrap it\n",
    "                return [data]\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: assume JSON Lines format\n",
    "            data = []\n",
    "            for line in text.splitlines():\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                data.append(json.loads(line))\n",
    "            return data\n",
    "\n",
    "\n",
    "def build_feature_matrix_from_records(records, feature_mode=\"img+text\",\n",
    "                                      id_key=\"id\", target_key=\"label\", is_train=True):\n",
    "    \"\"\"\n",
    "    Given a list of records like:\n",
    "    {\n",
    "      \"id\": \"a9d8c7...\",\n",
    "      \"label\": 0,\n",
    "      \"image_embedding\": [...512 floats...],\n",
    "      \"text_embedding\": [...512 floats...]\n",
    "    }\n",
    "    returns:\n",
    "      X: np.ndarray [n_samples, n_features]\n",
    "      y: np.ndarray [n_samples] (if is_train=True, else None)\n",
    "      ids: list of ids (for linking back to samples / submission)\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    features = []\n",
    "    labels = [] if is_train else None\n",
    "\n",
    "    for rec in records:\n",
    "        rid = rec[id_key]\n",
    "        img = rec.get(\"image_embedding\", [])\n",
    "        txt = rec.get(\"text_embedding\", [])\n",
    "\n",
    "        # Ensure they are np arrays\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        txt = np.array(txt, dtype=np.float32)\n",
    "\n",
    "        if feature_mode == \"img+text\":\n",
    "            feat = np.concatenate([img, txt], axis=0)\n",
    "        elif feature_mode == \"img_only\":\n",
    "            feat = img\n",
    "        elif feature_mode == \"text_only\":\n",
    "            feat = txt\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown FEATURE_MODE: {feature_mode}\")\n",
    "\n",
    "        ids.append(rid)\n",
    "        features.append(feat)\n",
    "        if is_train:\n",
    "            labels.append(rec[target_key])\n",
    "\n",
    "    X = np.stack(features, axis=0)\n",
    "    y = np.array(labels, dtype=np.int64) if is_train else None\n",
    "\n",
    "    return X, y, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bc650d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3: Load Train & Test\n",
    "# =========================\n",
    "\n",
    "def load_train_test(config=CONFIG):\n",
    "    data_dir = Path(config[\"DATA_DIR\"])\n",
    "    train_path = data_dir / config[\"TRAIN_FILENAME\"]\n",
    "    test_path = data_dir / config[\"TEST_FILENAME\"]\n",
    "\n",
    "    print(f\"Loading train from: {train_path}\")\n",
    "    train_records = load_json_list(train_path)\n",
    "\n",
    "    print(f\"Loading test from:  {test_path}\")\n",
    "    test_records = load_json_list(test_path)\n",
    "\n",
    "    print(f\"Train samples: {len(train_records)}\")\n",
    "    print(f\"Test samples:  {len(test_records)}\")\n",
    "\n",
    "    X_train, y_train, train_ids = build_feature_matrix_from_records(\n",
    "        train_records,\n",
    "        feature_mode=config[\"FEATURE_MODE\"],\n",
    "        id_key=config[\"ID_COLUMN\"],\n",
    "        target_key=config[\"TARGET_COLUMN\"],\n",
    "        is_train=True,\n",
    "    )\n",
    "\n",
    "    X_test, _, test_ids = build_feature_matrix_from_records(\n",
    "        test_records,\n",
    "        feature_mode=config[\"FEATURE_MODE\"],\n",
    "        id_key=config[\"ID_COLUMN\"],\n",
    "        target_key=config[\"TARGET_COLUMN\"],  # ignored when is_train=False\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # (n_train, n_features)\n",
    "    print(f\"X_test shape:  {X_test.shape}\")   # (n_test, n_features)\n",
    "\n",
    "    # Basic label distribution & NaN checks\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    label_dist = dict(zip(unique, counts))\n",
    "    print(\"Label distribution (train):\", label_dist)\n",
    "\n",
    "    # NaN / inf check\n",
    "    n_nan_train = np.isnan(X_train).sum()\n",
    "    n_nan_test = np.isnan(X_test).sum()\n",
    "    n_inf_train = np.isinf(X_train).sum()\n",
    "    n_inf_test = np.isinf(X_test).sum()\n",
    "\n",
    "    print(f\"NaNs in X_train: {n_nan_train}, NaNs in X_test: {n_nan_test}\")\n",
    "    print(f\"Infs in X_train: {n_inf_train}, Infs in X_test: {n_inf_test}\")\n",
    "\n",
    "    return X_train, y_train, train_ids, X_test, test_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd158e76",
   "metadata": {},
   "source": [
    "## load train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1da44291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train from: data\\train_p1.json\n",
      "Loading test from:  data\\test.json\n",
      "Train samples: 1530\n",
      "Test samples:  500\n",
      "X_train shape: (1530, 1024)\n",
      "X_test shape:  (500, 1024)\n",
      "Label distribution (train): {np.int64(0): np.int64(1326), np.int64(1): np.int64(204)}\n",
      "NaNs in X_train: 0, NaNs in X_test: 0\n",
      "Infs in X_train: 0, Infs in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Actually load the data (you can comment this out during debugging if needed)\n",
    "X_train, y_train, train_ids, X_test, test_ids = load_train_test(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722d5f6",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c1adb",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff42f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 4: Preprocessing (Imputer + Scaler + PCA)\n",
    "# =========================\n",
    "\n",
    "def build_imputer(config):\n",
    "    \"\"\"Return an sklearn SimpleImputer or None based on CONFIG.\"\"\"\n",
    "    strategy = config[\"MISSING_VALUE_STRATEGY\"]\n",
    "\n",
    "    if strategy == \"none\":\n",
    "        return None\n",
    "    elif strategy == \"mean\":\n",
    "        return SimpleImputer(strategy=\"mean\")\n",
    "    elif strategy == \"median\":\n",
    "        return SimpleImputer(strategy=\"median\")\n",
    "    elif strategy in (\"constant\", \"zero\"):\n",
    "        fill_value = 0.0 if strategy == \"zero\" else config[\"MISSING_FILL_VALUE\"]\n",
    "        return SimpleImputer(strategy=\"constant\", fill_value=fill_value)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown MISSING_VALUE_STRATEGY: {strategy}\")\n",
    "\n",
    "\n",
    "def build_scaler(config):\n",
    "    \"\"\"Return a scaler/normalizer or None based on CONFIG['SCALER_TYPE'].\"\"\"\n",
    "    scaler_type = config[\"SCALER_TYPE\"]\n",
    "\n",
    "    if scaler_type == \"none\":\n",
    "        return None\n",
    "    elif scaler_type == \"standard\":\n",
    "        # zero mean, unit variance per feature\n",
    "        return StandardScaler()\n",
    "    elif scaler_type == \"minmax\":\n",
    "        # scales each feature to [0, 1]\n",
    "        return MinMaxScaler()\n",
    "    elif scaler_type == \"maxabs\":\n",
    "        # scales each feature to [-1, 1] based on max abs value\n",
    "        return MaxAbsScaler()\n",
    "    elif scaler_type == \"robust\":\n",
    "        # robust to outliers (uses median & IQR)\n",
    "        return RobustScaler()\n",
    "    elif scaler_type == \"l2_norm\":\n",
    "        # normalizes each sample (row) to unit L2 norm\n",
    "        # (good when you care about direction, not magnitude)\n",
    "        return Normalizer(norm=\"l2\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown SCALER_TYPE: {scaler_type}\")\n",
    "\n",
    "\n",
    "def build_pca(config, n_features: int):\n",
    "    \"\"\"Return a PCA object or None based on CONFIG.\"\"\"\n",
    "    if not config[\"USE_PCA\"]:\n",
    "        return None\n",
    "\n",
    "    n_components = config[\"PCA_N_COMPONENTS\"]\n",
    "    # Safety: can't have more components than features\n",
    "    n_components = min(n_components, n_features)\n",
    "\n",
    "    pca = PCA(\n",
    "        n_components=n_components,\n",
    "        whiten=config[\"PCA_WHITEN\"],\n",
    "        svd_solver=config[\"PCA_SVD_SOLVER\"],\n",
    "        random_state=config[\"PCA_RANDOM_STATE\"],\n",
    "    )\n",
    "    return pca\n",
    "\n",
    "\n",
    "def fit_preprocessor(X_train: np.ndarray, config=CONFIG):\n",
    "    \"\"\"\n",
    "    Fit imputer, scaler, and PCA on X_train according to CONFIG.\n",
    "    Returns:\n",
    "      X_train_proc: transformed training features\n",
    "      preprocessors: dict with keys 'imputer', 'scaler', 'pca'\n",
    "    \"\"\"\n",
    "    X_proc = X_train.astype(np.float32, copy=True)\n",
    "\n",
    "    # 1) Missing values\n",
    "    imputer = build_imputer(config)\n",
    "    if imputer is not None:\n",
    "        X_proc = imputer.fit_transform(X_proc)\n",
    "    else:\n",
    "        # If we claim there are no missing values, assert this (fail fast)\n",
    "        if np.isnan(X_proc).any() or np.isinf(X_proc).any():\n",
    "            raise ValueError(\n",
    "                \"NaN or inf detected in X_train but MISSING_VALUE_STRATEGY='none'. \"\n",
    "                \"Change strategy to 'mean'/'median'/'zero' or clean data.\"\n",
    "            )\n",
    "\n",
    "    # 2) Scaling / normalization\n",
    "    scaler = build_scaler(config)\n",
    "    if scaler is not None:\n",
    "        X_proc = scaler.fit_transform(X_proc)\n",
    "\n",
    "    # 3) PCA\n",
    "    pca = build_pca(config, n_features=X_proc.shape[1])\n",
    "    if pca is not None:\n",
    "        X_proc = pca.fit_transform(X_proc)\n",
    "        print(f\"PCA applied: new shape = {X_proc.shape}\")\n",
    "    else:\n",
    "        print(\"PCA not used; shape remains:\", X_proc.shape)\n",
    "\n",
    "    preprocessors = {\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"pca\": pca,\n",
    "    }\n",
    "    return X_proc, preprocessors\n",
    "\n",
    "\n",
    "def transform_with_preprocessor(X: np.ndarray, preprocessors: dict):\n",
    "    \"\"\"\n",
    "    Apply fitted imputer, scaler, and PCA to new data (val/test).\n",
    "    X: np.ndarray [n_samples, n_features_orig]\n",
    "    preprocessors: dict from fit_preprocessor()\n",
    "\n",
    "    Returns:\n",
    "      X_proc: transformed X\n",
    "    \"\"\"\n",
    "    X_proc = X.astype(np.float32, copy=True)\n",
    "\n",
    "    imputer = preprocessors.get(\"imputer\")\n",
    "    scaler = preprocessors.get(\"scaler\")\n",
    "    pca = preprocessors.get(\"pca\")\n",
    "\n",
    "    # 1) Missing values\n",
    "    if imputer is not None:\n",
    "        X_proc = imputer.transform(X_proc)\n",
    "    else:\n",
    "        if np.isnan(X_proc).any() or np.isinf(X_proc).any():\n",
    "            raise ValueError(\n",
    "                \"NaN or inf detected in X but no imputer fitted (MISSING_VALUE_STRATEGY='none').\"\n",
    "            )\n",
    "\n",
    "    # 2) Scaling / normalization\n",
    "    if scaler is not None:\n",
    "        X_proc = scaler.transform(X_proc)\n",
    "\n",
    "    # 3) PCA\n",
    "    if pca is not None:\n",
    "        X_proc = pca.transform(X_proc)\n",
    "\n",
    "    return X_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afefabd1",
   "metadata": {},
   "source": [
    "## preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "429ac3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA applied: new shape = (1530, 64)\n",
      "X_train_proc shape: (1530, 64)\n",
      "X_test_proc shape:  (500, 64)\n"
     ]
    }
   ],
   "source": [
    "# Example: preprocess the full train & test according to CONFIG\n",
    "X_train_proc, PREPROCESSORS = fit_preprocessor(X_train, CONFIG)\n",
    "X_test_proc = transform_with_preprocessor(X_test, PREPROCESSORS)\n",
    "\n",
    "print(\"X_train_proc shape:\", X_train_proc.shape)\n",
    "print(\"X_test_proc shape: \", X_test_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f86c7",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143b0d8",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bb604e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 5: Cross-validation utilities\n",
    "# =========================\n",
    "\n",
    "def get_cv_splits(y, config=CONFIG):\n",
    "    \"\"\"\n",
    "    Build K-fold (or StratifiedKFold) splits based on CONFIG.\n",
    "    Returns a list of (train_idx, val_idx) tuples.\n",
    "    \"\"\"\n",
    "    n_folds = config[\"N_FOLDS\"]\n",
    "    shuffle = config[\"SHUFFLE_FOLDS\"]\n",
    "    random_state = config[\"FOLDS_RANDOM_STATE\"]\n",
    "\n",
    "    if config[\"USE_STRATIFIED_KFOLD\"]:\n",
    "        splitter = StratifiedKFold(\n",
    "            n_splits=n_folds,\n",
    "            shuffle=shuffle,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        splits = list(splitter.split(np.zeros_like(y), y))\n",
    "    else:\n",
    "        splitter = KFold(\n",
    "            n_splits=n_folds,\n",
    "            shuffle=shuffle,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        splits = list(splitter.split(np.zeros_like(y)))\n",
    "    return splits\n",
    "\n",
    "\n",
    "def run_cv_and_fit_full(build_model_fn, X, y, X_test, config=CONFIG, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    - build_model_fn: function that returns a *fresh* untrained model object\n",
    "    - X, y: preprocessed training data (e.g. X_train_proc, y_train)\n",
    "    - X_test: preprocessed test data (e.g. X_test_proc)\n",
    "    - config: CONFIG dict\n",
    "    - model_name: just for printing\n",
    "\n",
    "    Returns:\n",
    "      results: dict with:\n",
    "        - \"cv_f1_scores\"\n",
    "        - \"cv_f1_mean\"\n",
    "        - \"cv_f1_std\"\n",
    "        - \"test_pred\" (binary 0/1)\n",
    "        - \"test_proba\" (or None if not available)\n",
    "        - \"train_time_sec\"\n",
    "    \"\"\"\n",
    "    splits = get_cv_splits(y, config)\n",
    "    threshold = config[\"DECISION_THRESHOLD\"]\n",
    "\n",
    "    fold_scores = []\n",
    "    start_time = time()\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(splits, start=1):\n",
    "        print(f\"\\n=== {model_name}: Fold {fold}/{len(splits)} ===\")\n",
    "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "        model = build_model_fn()\n",
    "\n",
    "        # Fit\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        # Predict on validation\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            val_proba = model.predict_proba(X_val)[:, 1]\n",
    "            val_pred = (val_proba >= threshold).astype(int)\n",
    "        else:\n",
    "            # fallback: direct class prediction\n",
    "            val_pred = model.predict(X_val)\n",
    "\n",
    "        f1 = f1_score(y_val, val_pred, average=\"macro\")\n",
    "        print(f\"Fold {fold} F1 Macro: {f1:.4f}\")\n",
    "        fold_scores.append(f1)\n",
    "\n",
    "    train_time_sec = time() - start_time\n",
    "\n",
    "    cv_mean = float(np.mean(fold_scores))\n",
    "    cv_std = float(np.std(fold_scores))\n",
    "    print(\"\\n=== CV Summary ===\")\n",
    "    print(f\"{model_name}: F1 Macro mean = {cv_mean:.4f}, std = {cv_std:.4f}\")\n",
    "    print(f\"Training + CV time: {train_time_sec:.1f} sec\")\n",
    "\n",
    "    # Fit on full training data and predict on test\n",
    "    full_model = build_model_fn()\n",
    "    full_model.fit(X, y)\n",
    "\n",
    "    test_proba = None\n",
    "    if hasattr(full_model, \"predict_proba\"):\n",
    "        test_proba = full_model.predict_proba(X_test)[:, 1]\n",
    "        test_pred = (test_proba >= threshold).astype(int)\n",
    "    else:\n",
    "        test_pred = full_model.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        \"cv_f1_scores\": fold_scores,\n",
    "        \"cv_f1_mean\": cv_mean,\n",
    "        \"cv_f1_std\": cv_std,\n",
    "        \"test_pred\": test_pred.astype(int),\n",
    "        \"test_proba\": test_proba,\n",
    "        \"train_time_sec\": train_time_sec,\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a0ea428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 6: Submission helper\n",
    "# =========================\n",
    "\n",
    "def save_submission(test_ids, test_pred, config=CONFIG, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    test_ids: list of ids from test.json (must match Kaggle id column)\n",
    "    test_pred: numpy array of 0/1 predictions\n",
    "    suffix: e.g. \"lgbm_v1\", \"xgb_pca256\", etc.\n",
    "    \"\"\"\n",
    "    df_sub = pd.DataFrame({\n",
    "        \"row_id\": test_ids,       # Kaggle expects row_id as per manual\n",
    "        \"target\": test_pred.astype(int),\n",
    "    })\n",
    "    out_dir = Path(config[\"SUBMISSION_OUTPUT_DIR\"])\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    exp_name = f\"{config['MODEL_FAMILY']}_{config['EXPERIMENTER']}\"\n",
    "    if suffix:\n",
    "        filename = f\"{exp_name}_{suffix}.csv\"\n",
    "    else:\n",
    "        filename = f\"{exp_name}.csv\"\n",
    "\n",
    "    out_path = out_dir / filename\n",
    "    df_sub.to_csv(out_path, index=False)\n",
    "    print(f\"Saved submission to: {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32d15607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell X: Logging helpers\n",
    "# =========================\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "def _append_row_to_csv(path: Path, fieldnames, row_dict):\n",
    "    \"\"\"\n",
    "    Internal helper: append a row to a CSV.\n",
    "    - Creates file with header if it doesn't exist.\n",
    "    - Preserves column order via fieldnames.\n",
    "    \"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    file_exists = path.exists()\n",
    "\n",
    "    with path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "\n",
    "def log_experiment_master(\n",
    "    config,\n",
    "    results,\n",
    "    submission_path=None,\n",
    "    notes=\"\",\n",
    "    public_lb_f1_macro=None,\n",
    "    private_lb_f1_macro=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs a SINGLE row into logs/experiments_log.csv\n",
    "\n",
    "    config: CONFIG dict\n",
    "    results: dict from run_cv_and_fit_full (must contain:\n",
    "             \"cv_f1_mean\", \"cv_f1_std\", \"cv_f1_scores\", \"train_time_sec\")\n",
    "    submission_path: path of the submission CSV (string or Path)\n",
    "    notes: free text (e.g. \"no PCA; SCALER=standard; first lgbm run\")\n",
    "    public_lb_f1_macro / private_lb_f1_macro:\n",
    "        - you can leave as None for now and fill manually in Google Sheet later.\n",
    "    \"\"\"\n",
    "\n",
    "    log_path = Path(config[\"LOGS_DIR\"]) / f\"experiments_log_{config[\"EXPERIMENTER\"]}.csv\"\n",
    "\n",
    "    # Master columns (same for ALL models)\n",
    "    fieldnames = [\n",
    "        \"EXP_NAME\",\n",
    "        \"EXPERIMENTER\",\n",
    "        \"MODEL_FAMILY\",\n",
    "        \"FEATURE_MODE\",\n",
    "\n",
    "        \"SEED\",\n",
    "        \"N_FOLDS\",\n",
    "        \"VALIDATION_SCHEME\",\n",
    "        \"VAL_SIZE\",\n",
    "        \"VAL_STRATIFY\",\n",
    "        \"USE_STRATIFIED_KFOLD\",\n",
    "        \"SHUFFLE_FOLDS\",\n",
    "        \"FOLDS_RANDOM_STATE\",\n",
    "\n",
    "        \"SCALER_TYPE\",\n",
    "        \"MISSING_VALUE_STRATEGY\",\n",
    "        \"MISSING_FILL_VALUE\",\n",
    "\n",
    "        \"USE_PCA\",\n",
    "        \"PCA_N_COMPONENTS\",\n",
    "        \"PCA_WHITEN\",\n",
    "        \"PCA_SVD_SOLVER\",\n",
    "\n",
    "        \"IMBALANCE_MODE\",\n",
    "        \"DECISION_THRESHOLD\",\n",
    "\n",
    "        \"train_size\",\n",
    "        \"cv_f1_macro_mean\",\n",
    "        \"cv_f1_macro_std\",\n",
    "        \"cv_f1_macro_per_fold\",   # we store as JSON string\n",
    "        \"public_lb_f1_macro\",\n",
    "        \"private_lb_f1_macro\",\n",
    "\n",
    "        \"train_time_sec\",\n",
    "        \"submission_file\",\n",
    "        \"notes\",\n",
    "\n",
    "        \"config_json\",            # full CONFIG snapshot as JSON\n",
    "    ]\n",
    "\n",
    "    row = {\n",
    "        \"EXP_NAME\": config[\"EXP_NAME\"],\n",
    "        \"EXPERIMENTER\": config[\"EXPERIMENTER\"],\n",
    "        \"MODEL_FAMILY\": config[\"MODEL_FAMILY\"],\n",
    "        \"FEATURE_MODE\": config[\"FEATURE_MODE\"],\n",
    "\n",
    "        \"SEED\": config[\"SEED\"],\n",
    "        \"N_FOLDS\": config[\"N_FOLDS\"],\n",
    "        \"VALIDATION_SCHEME\": config[\"VALIDATION_SCHEME\"],\n",
    "        \"VAL_SIZE\": config[\"VAL_SIZE\"],\n",
    "        \"VAL_STRATIFY\": config[\"VAL_STRATIFY\"],\n",
    "        \"USE_STRATIFIED_KFOLD\": config[\"USE_STRATIFIED_KFOLD\"],\n",
    "        \"SHUFFLE_FOLDS\": config[\"SHUFFLE_FOLDS\"],\n",
    "        \"FOLDS_RANDOM_STATE\": config[\"FOLDS_RANDOM_STATE\"],\n",
    "\n",
    "        \"SCALER_TYPE\": config[\"SCALER_TYPE\"],\n",
    "        \"MISSING_VALUE_STRATEGY\": config[\"MISSING_VALUE_STRATEGY\"],\n",
    "        \"MISSING_FILL_VALUE\": config[\"MISSING_FILL_VALUE\"],\n",
    "\n",
    "        \"USE_PCA\": config[\"USE_PCA\"],\n",
    "        \"PCA_N_COMPONENTS\": config[\"PCA_N_COMPONENTS\"],\n",
    "        \"PCA_WHITEN\": config[\"PCA_WHITEN\"],\n",
    "        \"PCA_SVD_SOLVER\": config[\"PCA_SVD_SOLVER\"],\n",
    "\n",
    "        \"IMBALANCE_MODE\": config[\"IMBALANCE_MODE\"],\n",
    "        \"DECISION_THRESHOLD\": config[\"DECISION_THRESHOLD\"],\n",
    "\n",
    "        \"train_size\": int(len(y_train)),  # global y_train\n",
    "\n",
    "        \"cv_f1_macro_mean\": float(results[\"cv_f1_mean\"]),\n",
    "        \"cv_f1_macro_std\": float(results[\"cv_f1_std\"]),\n",
    "        \"cv_f1_macro_per_fold\": json.dumps(results[\"cv_f1_scores\"]),\n",
    "\n",
    "        \"public_lb_f1_macro\": (\n",
    "            float(public_lb_f1_macro) if public_lb_f1_macro is not None else \"\"\n",
    "        ),\n",
    "        \"private_lb_f1_macro\": (\n",
    "            float(private_lb_f1_macro) if private_lb_f1_macro is not None else \"\"\n",
    "        ),\n",
    "\n",
    "        \"train_time_sec\": float(results[\"train_time_sec\"]),\n",
    "        \"submission_file\": str(submission_path) if submission_path is not None else \"\",\n",
    "        \"notes\": notes,\n",
    "\n",
    "        \"config_json\": json.dumps(config),\n",
    "    }\n",
    "\n",
    "    _append_row_to_csv(log_path, fieldnames, row)\n",
    "    print(f\"[LOG] Appended master experiment log → {log_path}\")\n",
    "\n",
    "\n",
    "def log_experiment_model_specific(\n",
    "    model_family,\n",
    "    config,\n",
    "    params_dict,\n",
    "    results,\n",
    "    submission_path=None,\n",
    "    notes=\"\",\n",
    "    public_lb_f1_macro=None,\n",
    "    private_lb_f1_macro=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs a SINGLE row into logs/<model_family>.csv\n",
    "\n",
    "    model_family: string, e.g. \"lightgbm\", \"xgboost\", \"mlp_early\", ...\n",
    "    config: CONFIG dict\n",
    "    params_dict: the hyperparameter dict used to construct the model\n",
    "                 (e.g. the 'params' dict inside build_lgbm_model)\n",
    "    results: dict from run_cv_and_fit_full\n",
    "    submission_path: path to submission CSV\n",
    "    \"\"\"\n",
    "\n",
    "    model_log_path = Path(config[\"LOGS_DIR\"]) / f\"{model_family}_{config[\"EXPERIMENTER\"]}.csv\"\n",
    "\n",
    "    # General columns we want in EVERY model-family sheet\n",
    "    general_cols = [\n",
    "        \"EXP_NAME\",\n",
    "        \"EXPERIMENTER\",\n",
    "        \"MODEL_FAMILY\",\n",
    "        \"SEED\",\n",
    "        \"N_FOLDS\",\n",
    "        \"FEATURE_MODE\",\n",
    "        \"SCALER_TYPE\",\n",
    "        \"USE_PCA\",\n",
    "        \"PCA_N_COMPONENTS\",\n",
    "        \"IMBALANCE_MODE\",\n",
    "        \"train_size\",\n",
    "        \"cv_f1_macro_mean\",\n",
    "        \"cv_f1_macro_std\",\n",
    "        \"public_lb_f1_macro\",\n",
    "        \"private_lb_f1_macro\",\n",
    "        \"train_time_sec\",\n",
    "        \"submission_file\",\n",
    "        \"notes\",\n",
    "    ]\n",
    "\n",
    "    # Model-specific hyperparam column names (sorted for a stable order)\n",
    "    param_cols = sorted(list(params_dict.keys()))\n",
    "\n",
    "    fieldnames = general_cols + param_cols\n",
    "\n",
    "    row = {\n",
    "        \"EXP_NAME\": config[\"EXP_NAME\"],\n",
    "        \"EXPERIMENTER\": config[\"EXPERIMENTER\"],\n",
    "        \"MODEL_FAMILY\": config[\"MODEL_FAMILY\"],\n",
    "        \"SEED\": config[\"SEED\"],\n",
    "        \"N_FOLDS\": config[\"N_FOLDS\"],\n",
    "        \"FEATURE_MODE\": config[\"FEATURE_MODE\"],\n",
    "        \"SCALER_TYPE\": config[\"SCALER_TYPE\"],\n",
    "        \"USE_PCA\": config[\"USE_PCA\"],\n",
    "        \"PCA_N_COMPONENTS\": config[\"PCA_N_COMPONENTS\"],\n",
    "        \"IMBALANCE_MODE\": config[\"IMBALANCE_MODE\"],\n",
    "        \"train_size\": int(len(y_train)),\n",
    "        \"cv_f1_macro_mean\": float(results[\"cv_f1_mean\"]),\n",
    "        \"cv_f1_macro_std\": float(results[\"cv_f1_std\"]),\n",
    "        \"public_lb_f1_macro\": (\n",
    "            float(public_lb_f1_macro) if public_lb_f1_macro is not None else \"\"\n",
    "        ),\n",
    "        \"private_lb_f1_macro\": (\n",
    "            float(private_lb_f1_macro) if private_lb_f1_macro is not None else \"\"\n",
    "        ),\n",
    "        \"train_time_sec\": float(results[\"train_time_sec\"]),\n",
    "        \"submission_file\": str(submission_path) if submission_path is not None else \"\",\n",
    "        \"notes\": notes,\n",
    "    }\n",
    "\n",
    "    # Add each hyperparam into the row\n",
    "    for k in param_cols:\n",
    "        row[k] = params_dict.get(k, \"\")\n",
    "\n",
    "    _append_row_to_csv(model_log_path, fieldnames, row)\n",
    "    print(f\"[LOG] Appended model-specific log → {model_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbbbd5",
   "metadata": {},
   "source": [
    "## train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b563eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LinearSVM: Fold 1/7 ===\n",
      "[LibLinear]Fold 1 F1 Macro: 0.6343\n",
      "\n",
      "=== LinearSVM: Fold 2/7 ===\n",
      "[LibLinear]Fold 2 F1 Macro: 0.6265\n",
      "\n",
      "=== LinearSVM: Fold 3/7 ===\n",
      "[LibLinear]Fold 3 F1 Macro: 0.6186\n",
      "\n",
      "=== LinearSVM: Fold 4/7 ===\n",
      "[LibLinear]Fold 4 F1 Macro: 0.6017\n",
      "\n",
      "=== LinearSVM: Fold 5/7 ===\n",
      "[LibLinear]Fold 5 F1 Macro: 0.6441\n",
      "\n",
      "=== LinearSVM: Fold 6/7 ===\n",
      "[LibLinear]Fold 6 F1 Macro: 0.6566\n",
      "\n",
      "=== LinearSVM: Fold 7/7 ===\n",
      "[LibLinear]Fold 7 F1 Macro: 0.6591\n",
      "\n",
      "=== CV Summary ===\n",
      "LinearSVM: F1 Macro mean = 0.6344, std = 0.0192\n",
      "Training + CV time: 0.2 sec\n",
      "[LibLinear]Saved submission to: submissions\\svm_Zuha.csv\n",
      "[LOG] Appended master experiment log → logs\\experiments_log_Zuha.csv\n",
      "[LOG] Appended model-specific log → logs\\svm_Zuha.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Linear SVM: params + builder + run + log\n",
    "# =========================\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def get_linear_svm_params(config=CONFIG):\n",
    "    \"\"\"\n",
    "    LinearSVC main parameters you can explore:\n",
    "\n",
    "    Core optimization / regularization:\n",
    "      - C                : float > 0\n",
    "                           • smaller → stronger regularization (simpler model)\n",
    "                           • typical grid: [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "      - penalty          : {\"l2\", \"l1\"}\n",
    "                           • \"l2\" is the standard choice\n",
    "                           • \"l1\" only allowed with:\n",
    "                               penalty=\"l1\", loss=\"squared_hinge\", dual=False\n",
    "\n",
    "      - loss             : {\"squared_hinge\", \"hinge\"}\n",
    "                           • \"squared_hinge\" (default), smoother, more common\n",
    "                           • \"hinge\" is the original SVM hinge loss\n",
    "\n",
    "      - dual             : bool\n",
    "                           • True  → solves the dual problem (better when n_samples > n_features)\n",
    "                           • False → solves the primal (often better when n_features > n_samples)\n",
    "                           • IMPORTANT:\n",
    "                               - If penalty=\"l1\", loss must be \"squared_hinge\" and dual=False\n",
    "\n",
    "      - tol              : float > 0\n",
    "                           • tolerance for stopping\n",
    "                           • smaller (1e-5) = more precise but slower\n",
    "                           • typical: [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "    Data / formulation:\n",
    "      - fit_intercept    : {True, False}\n",
    "                           • True  → learns an intercept/bias term\n",
    "                           • False → forces hyperplane through origin\n",
    "\n",
    "      - intercept_scaling: float (only used when fit_intercept=True and solver is liblinear-like)\n",
    "                           • default 1.0; usually you can leave this\n",
    "\n",
    "      - class_weight     : {None, \"balanced\", dict}\n",
    "                           • None       → no reweighting\n",
    "                           • \"balanced\" → inverse-frequency weighting per class\n",
    "                           • dict       → custom weights per class\n",
    "\n",
    "      - multi_class      : {\"ovr\"}\n",
    "                           • LinearSVC only supports \"ovr\" (one-vs-rest)\n",
    "                           • for binary problem this doesn’t matter\n",
    "\n",
    "    Misc:\n",
    "      - max_iter         : int\n",
    "                           • max iterations; increase if you get convergence warnings\n",
    "                           • typical: [1000, 5000, 10000, 20000]\n",
    "\n",
    "      - random_state     : int or None\n",
    "                           • controls shuffling in the optimizer (for reproducibility)\n",
    "\n",
    "      - verbose          : int\n",
    "                           • 0 = silent, >0 = prints optimization info (you can ignore)\n",
    "    \"\"\"\n",
    "\n",
    "    # ⚠️ Choose a *valid* combination here.\n",
    "    # Example 1: Standard and safe\n",
    "    penalty = \"l1\"              # \"l2\" or \"l1\"\n",
    "    loss = \"squared_hinge\"      # \"squared_hinge\" or \"hinge\"\n",
    "    dual = False                 # True or False (see notes above)\n",
    "\n",
    "    # Example 2: enable l1 penalty (UNCOMMENT IF YOU WANT TO TRY IT)\n",
    "    # penalty = \"l1\"\n",
    "    # loss = \"squared_hinge\"    # must be \"squared_hinge\" with l1\n",
    "    # dual = False              # must be False with l1\n",
    "\n",
    "    params = {\n",
    "        # --- core optimization ---\n",
    "        \"C\": 8.0,                 # explore: 0.01, 0.1, 1.0, 10.0, 100.0\n",
    "        \"penalty\": penalty,        # \"l2\" (safer) or \"l1\" (sparser, with constraints)\n",
    "        \"loss\": loss,              # \"squared_hinge\" (default) or \"hinge\"\n",
    "        \"dual\": dual,              # True (when n_samples > n_features), False otherwise\n",
    "        \"tol\": 1e-4,               # explore: 1e-3, 1e-4, 1e-5\n",
    "\n",
    "        # --- data / formulation ---\n",
    "        \"fit_intercept\": True,     # True (default) or False\n",
    "        \"intercept_scaling\": 1.0,  # rarely changed; relevant when fit_intercept=True\n",
    "\n",
    "        \"multi_class\": \"ovr\",      # only \"ovr\" is supported for LinearSVC\n",
    "\n",
    "        # class imbalance\n",
    "        # None       → no weighting\n",
    "        # \"balanced\" → inverse-frequency per class\n",
    "        # dict       → custom weights e.g. {0: 1.0, 1: 2.0}\n",
    "        \"class_weight\": None,\n",
    "\n",
    "        # --- misc ---\n",
    "        \"max_iter\": 5000,         # if you see convergence warnings, increase this\n",
    "        \"random_state\": config[\"SEED\"],\n",
    "        \"verbose\": 1,\n",
    "    }\n",
    "\n",
    "    # hook to enable \"balanced\" from CONFIG\n",
    "    if config[\"IMBALANCE_MODE\"] == \"balanced\":\n",
    "        params[\"class_weight\"] = \"balanced\"\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def build_linear_svm_model(config=CONFIG):\n",
    "    params = get_linear_svm_params(config)\n",
    "    model = LinearSVC(**params)\n",
    "    return model\n",
    "\n",
    "# ======= RUN ONE LinearSVC EXPERIMENT =======\n",
    "\n",
    "linsvm_results = run_cv_and_fit_full(\n",
    "    build_linear_svm_model,\n",
    "    X_train_proc,\n",
    "    y_train,\n",
    "    X_test_proc,\n",
    "    CONFIG,\n",
    "    model_name=\"LinearSVM\",\n",
    ")\n",
    "\n",
    "sub_path = save_submission(\n",
    "    test_ids,\n",
    "    linsvm_results[\"test_pred\"],\n",
    "    CONFIG,\n",
    ")\n",
    "\n",
    "linsvm_params = get_linear_svm_params(CONFIG)\n",
    "\n",
    "log_experiment_master(\n",
    "    config=CONFIG,\n",
    "    results=linsvm_results,\n",
    "    submission_path=sub_path,\n",
    "    notes=\"LinearSVC + PCA256, standard scaler.\",\n",
    ")\n",
    "\n",
    "log_experiment_model_specific(\n",
    "    model_family=CONFIG[\"MODEL_FAMILY\"],\n",
    "    config=CONFIG,\n",
    "    params_dict=linsvm_params,\n",
    "    results=linsvm_results,\n",
    "    submission_path=sub_path,\n",
    "    notes=\"LinearSVC + PCA256, standard scaler.\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
