{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbd9859",
   "metadata": {},
   "source": [
    "Date: 16th Nov 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f2a4a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 1: Imports & CONFIG\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    MaxAbsScaler,\n",
    "    RobustScaler,\n",
    "    Normalizer,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4204ec6",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b14155",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # --- Meta / experiment info ---\n",
    "    \"EXP_NAME\": \"exp_lgbm_baseline_v1\",  # any short name for this run\n",
    "    \"EXPERIMENTER\": \"Zuha\",              # \"Zuha\" | \"Maryam\" | \"Maham\"\n",
    "    \"MODEL_FAMILY\": \"lightgbm\",          # \"lightgbm\" | \"xgboost\" | \"mlp_early\" | \"mlp_two_tower\"\n",
    "                                          # \"logreg\" | \"svm\" | \"rf\" | \"catboost\" | \"ensemble\"\n",
    "\n",
    "    # --- Paths (update DATA_DIR when you know Kaggle path) ---\n",
    "    # On Kaggle, you'll usually have something like: \"/kaggle/input/dss-ml-competition/\"\n",
    "    \"DATA_DIR\": \"./data\",                # folder containing train_part1.json & test.json\n",
    "    \"TRAIN_FILENAME\": \"train_part1.json\",\n",
    "    \"TEST_FILENAME\": \"test.json\",\n",
    "    \"SUBMISSION_OUTPUT_DIR\": \"./submissions\",  # where to save CSV submissions\n",
    "\n",
    "    # --- Target / ID / feature mode ---\n",
    "    \"TARGET_COLUMN\": \"label\",            # name of target in train JSON\n",
    "    \"ID_COLUMN\": \"id\",                   # for train: hashed id, for test: integer\n",
    "    \"FEATURE_MODE\": \"img+text\",          # \"img+text\" | \"img_only\" | \"text_only\"\n",
    "                                         # (later you can try ablations)\n",
    "\n",
    "    # --- Reproducibility ---\n",
    "    \"SEED\": 42,                          # any integer; try 42, 2025, 7, etc.\n",
    "    \"NP_RANDOM_SEED\": 42,                # keep same as SEED or change if you want\n",
    "\n",
    "    # --- Train / validation strategy (for local eval) ---\n",
    "    \"VALIDATION_SCHEME\": \"cv_only\",      # \"cv_only\"          → only K-fold CV\n",
    "                                         # \"holdout_only\"     → single train/val split\n",
    "                                         # \"holdout+cv\"       → both split + CV\n",
    "\n",
    "    # If using holdout (for VALIDATION_SCHEME != \"cv_only\"):\n",
    "    \"VAL_SIZE\": 0.2,                     # fraction of train data for validation (e.g. 0.1, 0.2)\n",
    "    \"VAL_STRATIFY\": True,                # True to stratify by label in train/val split\n",
    "    \"VAL_RANDOM_STATE\": 42,              # seed for train/val split\n",
    "\n",
    "    # --- Cross-validation (K-fold) ---\n",
    "    \"USE_STRATIFIED_KFOLD\": True,        # almost always True for classification\n",
    "    \"N_FOLDS\": 5,                        # typical: 3, 5, 7, 10\n",
    "    \"SHUFFLE_FOLDS\": True,               # shuffle before splitting into folds\n",
    "    \"FOLDS_RANDOM_STATE\": 42,            # seed for fold splitting\n",
    "\n",
    "    # --- Scaling / normalization ---\n",
    "    \"SCALER_TYPE\": \"standard\",           # \"none\"     → no scaling (good for tree models)\n",
    "                                         # \"standard\" → StandardScaler (zero mean, unit variance) NEEDED FOR MLP\n",
    "                                         # \"minmax\"   → MinMaxScaler (0–1)\n",
    "                                         # \"maxabs\"   → MaxAbsScaler ([-1, 1] for sparse-like)\n",
    "                                         # \"robust\"   → RobustScaler (robust to outliers)\n",
    "                                         # \"l2_norm\"  → normalize each sample to unit L2 norm\n",
    "\n",
    "    # --- Missing values handling (numeric) ---\n",
    "    \"MISSING_VALUE_STRATEGY\": \"none\",    # \"none\"     → assume no NaNs; just assert & crash if found\n",
    "                                         # \"mean\"     → fill NaNs with column means\n",
    "                                         # \"median\"   → fill with column medians\n",
    "                                         # \"constant\" → fill with constant (see MISSING_FILL_VALUE)\n",
    "                                         # \"zero\"     → fill with 0.0\n",
    "\n",
    "    \"MISSING_FILL_VALUE\": 0.0,           # used when strategy == \"constant\" or \"zero\"\n",
    "\n",
    "    # --- PCA dimensionality reduction ---\n",
    "    \"USE_PCA\": False,                    # True → apply PCA after scaling (if any)\n",
    "    \"PCA_N_COMPONENTS\": 256,             # e.g. 64, 128, 256, 512; <= 1024 total features\n",
    "    \"PCA_WHITEN\": False,                 # True → decorrelate & scale to unit variance\n",
    "    \"PCA_SVD_SOLVER\": \"auto\",            # \"auto\" | \"full\" | \"randomized\"\n",
    "    \"PCA_RANDOM_STATE\": 42,              # for randomized solver, etc.\n",
    "\n",
    "    # --- Class imbalance handling (general) ---\n",
    "    \"IMBALANCE_MODE\": \"none\",            # \"none\"           → do nothing\n",
    "                                         # \"balanced\"       → use class_weight='balanced' (for LR/SVM/MLP)\n",
    "                                         # \"scale_pos_weight\" → for tree models like XGB/LGB\n",
    "                                         # \"is_unbalance\"   → LightGBM's built-in option\n",
    "\n",
    "    # --- Threshold for converting probabilities → class labels ---\n",
    "    \"DECISION_THRESHOLD\": 0.5,           # default 0.5; you can tune (0.4–0.6, etc.)\n",
    "\n",
    "    # --- Logging / runs ---\n",
    "    \"SAVE_LOCAL_LOGS\": True,             # later: saves experiments_master.csv etc.\n",
    "    \"LOGS_DIR\": \"./logs\",                # folder for logs/CSVs if you want\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7be3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure directories exist\n",
    "Path(CONFIG[\"SUBMISSION_OUTPUT_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"LOGS_DIR\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40997a",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1ecec",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 2: Data Loading Utils\n",
    "# =========================\n",
    "\n",
    "def load_json_list(path):\n",
    "    \"\"\"\n",
    "    Loads a JSON file that contains either:\n",
    "    - a list of JSON objects, OR\n",
    "    - JSON lines (one object per line)\n",
    "    and returns a Python list of dicts.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "        if not text:\n",
    "            return []\n",
    "        # Try parse as a JSON array first\n",
    "        try:\n",
    "            data = json.loads(text)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            else:\n",
    "                # If it's a single object, wrap it\n",
    "                return [data]\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: assume JSON Lines format\n",
    "            data = []\n",
    "            for line in text.splitlines():\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                data.append(json.loads(line))\n",
    "            return data\n",
    "\n",
    "\n",
    "def build_feature_matrix_from_records(records, feature_mode=\"img+text\",\n",
    "                                      id_key=\"id\", target_key=\"label\", is_train=True):\n",
    "    \"\"\"\n",
    "    Given a list of records like:\n",
    "    {\n",
    "      \"id\": \"a9d8c7...\",\n",
    "      \"label\": 0,\n",
    "      \"image_embedding\": [...512 floats...],\n",
    "      \"text_embedding\": [...512 floats...]\n",
    "    }\n",
    "    returns:\n",
    "      X: np.ndarray [n_samples, n_features]\n",
    "      y: np.ndarray [n_samples] (if is_train=True, else None)\n",
    "      ids: list of ids (for linking back to samples / submission)\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    features = []\n",
    "    labels = [] if is_train else None\n",
    "\n",
    "    for rec in records:\n",
    "        rid = rec[id_key]\n",
    "        img = rec.get(\"image_embedding\", [])\n",
    "        txt = rec.get(\"text_embedding\", [])\n",
    "\n",
    "        # Ensure they are np arrays\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        txt = np.array(txt, dtype=np.float32)\n",
    "\n",
    "        if feature_mode == \"img+text\":\n",
    "            feat = np.concatenate([img, txt], axis=0)\n",
    "        elif feature_mode == \"img_only\":\n",
    "            feat = img\n",
    "        elif feature_mode == \"text_only\":\n",
    "            feat = txt\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown FEATURE_MODE: {feature_mode}\")\n",
    "\n",
    "        ids.append(rid)\n",
    "        features.append(feat)\n",
    "        if is_train:\n",
    "            labels.append(rec[target_key])\n",
    "\n",
    "    X = np.stack(features, axis=0)\n",
    "    y = np.array(labels, dtype=np.int64) if is_train else None\n",
    "\n",
    "    return X, y, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc650d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3: Load Train & Test\n",
    "# =========================\n",
    "\n",
    "def load_train_test(config=CONFIG):\n",
    "    data_dir = Path(config[\"DATA_DIR\"])\n",
    "    train_path = data_dir / config[\"TRAIN_FILENAME\"]\n",
    "    test_path = data_dir / config[\"TEST_FILENAME\"]\n",
    "\n",
    "    print(f\"Loading train from: {train_path}\")\n",
    "    train_records = load_json_list(train_path)\n",
    "\n",
    "    print(f\"Loading test from:  {test_path}\")\n",
    "    test_records = load_json_list(test_path)\n",
    "\n",
    "    print(f\"Train samples: {len(train_records)}\")\n",
    "    print(f\"Test samples:  {len(test_records)}\")\n",
    "\n",
    "    X_train, y_train, train_ids = build_feature_matrix_from_records(\n",
    "        train_records,\n",
    "        feature_mode=config[\"FEATURE_MODE\"],\n",
    "        id_key=config[\"ID_COLUMN\"],\n",
    "        target_key=config[\"TARGET_COLUMN\"],\n",
    "        is_train=True,\n",
    "    )\n",
    "\n",
    "    X_test, _, test_ids = build_feature_matrix_from_records(\n",
    "        test_records,\n",
    "        feature_mode=config[\"FEATURE_MODE\"],\n",
    "        id_key=config[\"ID_COLUMN\"],\n",
    "        target_key=config[\"TARGET_COLUMN\"],  # ignored when is_train=False\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # (n_train, n_features)\n",
    "    print(f\"X_test shape:  {X_test.shape}\")   # (n_test, n_features)\n",
    "\n",
    "    # Basic label distribution & NaN checks\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    label_dist = dict(zip(unique, counts))\n",
    "    print(\"Label distribution (train):\", label_dist)\n",
    "\n",
    "    # NaN / inf check\n",
    "    n_nan_train = np.isnan(X_train).sum()\n",
    "    n_nan_test = np.isnan(X_test).sum()\n",
    "    n_inf_train = np.isinf(X_train).sum()\n",
    "    n_inf_test = np.isinf(X_test).sum()\n",
    "\n",
    "    print(f\"NaNs in X_train: {n_nan_train}, NaNs in X_test: {n_nan_test}\")\n",
    "    print(f\"Infs in X_train: {n_inf_train}, Infs in X_test: {n_inf_test}\")\n",
    "\n",
    "    return X_train, y_train, train_ids, X_test, test_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd158e76",
   "metadata": {},
   "source": [
    "## load train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da44291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually load the data (you can comment this out during debugging if needed)\n",
    "X_train, y_train, train_ids, X_test, test_ids = load_train_test(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722d5f6",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c1adb",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 4: Preprocessing (Imputer + Scaler + PCA)\n",
    "# =========================\n",
    "\n",
    "def build_imputer(config):\n",
    "    \"\"\"Return an sklearn SimpleImputer or None based on CONFIG.\"\"\"\n",
    "    strategy = config[\"MISSING_VALUE_STRATEGY\"]\n",
    "\n",
    "    if strategy == \"none\":\n",
    "        return None\n",
    "    elif strategy == \"mean\":\n",
    "        return SimpleImputer(strategy=\"mean\")\n",
    "    elif strategy == \"median\":\n",
    "        return SimpleImputer(strategy=\"median\")\n",
    "    elif strategy in (\"constant\", \"zero\"):\n",
    "        fill_value = 0.0 if strategy == \"zero\" else config[\"MISSING_FILL_VALUE\"]\n",
    "        return SimpleImputer(strategy=\"constant\", fill_value=fill_value)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown MISSING_VALUE_STRATEGY: {strategy}\")\n",
    "\n",
    "\n",
    "def build_scaler(config):\n",
    "    \"\"\"Return a scaler/normalizer or None based on CONFIG['SCALER_TYPE'].\"\"\"\n",
    "    scaler_type = config[\"SCALER_TYPE\"]\n",
    "\n",
    "    if scaler_type == \"none\":\n",
    "        return None\n",
    "    elif scaler_type == \"standard\":\n",
    "        # zero mean, unit variance per feature\n",
    "        return StandardScaler()\n",
    "    elif scaler_type == \"minmax\":\n",
    "        # scales each feature to [0, 1]\n",
    "        return MinMaxScaler()\n",
    "    elif scaler_type == \"maxabs\":\n",
    "        # scales each feature to [-1, 1] based on max abs value\n",
    "        return MaxAbsScaler()\n",
    "    elif scaler_type == \"robust\":\n",
    "        # robust to outliers (uses median & IQR)\n",
    "        return RobustScaler()\n",
    "    elif scaler_type == \"l2_norm\":\n",
    "        # normalizes each sample (row) to unit L2 norm\n",
    "        # (good when you care about direction, not magnitude)\n",
    "        return Normalizer(norm=\"l2\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown SCALER_TYPE: {scaler_type}\")\n",
    "\n",
    "\n",
    "def build_pca(config, n_features: int):\n",
    "    \"\"\"Return a PCA object or None based on CONFIG.\"\"\"\n",
    "    if not config[\"USE_PCA\"]:\n",
    "        return None\n",
    "\n",
    "    n_components = config[\"PCA_N_COMPONENTS\"]\n",
    "    # Safety: can't have more components than features\n",
    "    n_components = min(n_components, n_features)\n",
    "\n",
    "    pca = PCA(\n",
    "        n_components=n_components,\n",
    "        whiten=config[\"PCA_WHITEN\"],\n",
    "        svd_solver=config[\"PCA_SVD_SOLVER\"],\n",
    "        random_state=config[\"PCA_RANDOM_STATE\"],\n",
    "    )\n",
    "    return pca\n",
    "\n",
    "\n",
    "def fit_preprocessor(X_train: np.ndarray, config=CONFIG):\n",
    "    \"\"\"\n",
    "    Fit imputer, scaler, and PCA on X_train according to CONFIG.\n",
    "    Returns:\n",
    "      X_train_proc: transformed training features\n",
    "      preprocessors: dict with keys 'imputer', 'scaler', 'pca'\n",
    "    \"\"\"\n",
    "    X_proc = X_train.astype(np.float32, copy=True)\n",
    "\n",
    "    # 1) Missing values\n",
    "    imputer = build_imputer(config)\n",
    "    if imputer is not None:\n",
    "        X_proc = imputer.fit_transform(X_proc)\n",
    "    else:\n",
    "        # If we claim there are no missing values, assert this (fail fast)\n",
    "        if np.isnan(X_proc).any() or np.isinf(X_proc).any():\n",
    "            raise ValueError(\n",
    "                \"NaN or inf detected in X_train but MISSING_VALUE_STRATEGY='none'. \"\n",
    "                \"Change strategy to 'mean'/'median'/'zero' or clean data.\"\n",
    "            )\n",
    "\n",
    "    # 2) Scaling / normalization\n",
    "    scaler = build_scaler(config)\n",
    "    if scaler is not None:\n",
    "        X_proc = scaler.fit_transform(X_proc)\n",
    "\n",
    "    # 3) PCA\n",
    "    pca = build_pca(config, n_features=X_proc.shape[1])\n",
    "    if pca is not None:\n",
    "        X_proc = pca.fit_transform(X_proc)\n",
    "        print(f\"PCA applied: new shape = {X_proc.shape}\")\n",
    "    else:\n",
    "        print(\"PCA not used; shape remains:\", X_proc.shape)\n",
    "\n",
    "    preprocessors = {\n",
    "        \"imputer\": imputer,\n",
    "        \"scaler\": scaler,\n",
    "        \"pca\": pca,\n",
    "    }\n",
    "    return X_proc, preprocessors\n",
    "\n",
    "\n",
    "def transform_with_preprocessor(X: np.ndarray, preprocessors: dict):\n",
    "    \"\"\"\n",
    "    Apply fitted imputer, scaler, and PCA to new data (val/test).\n",
    "    X: np.ndarray [n_samples, n_features_orig]\n",
    "    preprocessors: dict from fit_preprocessor()\n",
    "\n",
    "    Returns:\n",
    "      X_proc: transformed X\n",
    "    \"\"\"\n",
    "    X_proc = X.astype(np.float32, copy=True)\n",
    "\n",
    "    imputer = preprocessors.get(\"imputer\")\n",
    "    scaler = preprocessors.get(\"scaler\")\n",
    "    pca = preprocessors.get(\"pca\")\n",
    "\n",
    "    # 1) Missing values\n",
    "    if imputer is not None:\n",
    "        X_proc = imputer.transform(X_proc)\n",
    "    else:\n",
    "        if np.isnan(X_proc).any() or np.isinf(X_proc).any():\n",
    "            raise ValueError(\n",
    "                \"NaN or inf detected in X but no imputer fitted (MISSING_VALUE_STRATEGY='none').\"\n",
    "            )\n",
    "\n",
    "    # 2) Scaling / normalization\n",
    "    if scaler is not None:\n",
    "        X_proc = scaler.transform(X_proc)\n",
    "\n",
    "    # 3) PCA\n",
    "    if pca is not None:\n",
    "        X_proc = pca.transform(X_proc)\n",
    "\n",
    "    return X_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afefabd1",
   "metadata": {},
   "source": [
    "## preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ac3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: preprocess the full train & test according to CONFIG\n",
    "X_train_proc, PREPROCESSORS = fit_preprocessor(X_train, CONFIG)\n",
    "X_test_proc = transform_with_preprocessor(X_test, PREPROCESSORS)\n",
    "\n",
    "print(\"X_train_proc shape:\", X_train_proc.shape)\n",
    "print(\"X_test_proc shape: \", X_test_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f86c7",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143b0d8",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb604e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 5: Cross-validation utilities\n",
    "# =========================\n",
    "\n",
    "def get_cv_splits(y, config=CONFIG):\n",
    "    \"\"\"\n",
    "    Build K-fold (or StratifiedKFold) splits based on CONFIG.\n",
    "    Returns a list of (train_idx, val_idx) tuples.\n",
    "    \"\"\"\n",
    "    n_folds = config[\"N_FOLDS\"]\n",
    "    shuffle = config[\"SHUFFLE_FOLDS\"]\n",
    "    random_state = config[\"FOLDS_RANDOM_STATE\"]\n",
    "\n",
    "    if config[\"USE_STRATIFIED_KFOLD\"]:\n",
    "        splitter = StratifiedKFold(\n",
    "            n_splits=n_folds,\n",
    "            shuffle=shuffle,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        splits = list(splitter.split(np.zeros_like(y), y))\n",
    "    else:\n",
    "        splitter = KFold(\n",
    "            n_splits=n_folds,\n",
    "            shuffle=shuffle,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        splits = list(splitter.split(np.zeros_like(y)))\n",
    "    return splits\n",
    "\n",
    "\n",
    "def run_cv_and_fit_full(build_model_fn, X, y, X_test, config=CONFIG, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    - build_model_fn: function that returns a *fresh* untrained model object\n",
    "    - X, y: preprocessed training data (e.g. X_train_proc, y_train)\n",
    "    - X_test: preprocessed test data (e.g. X_test_proc)\n",
    "    - config: CONFIG dict\n",
    "    - model_name: just for printing\n",
    "\n",
    "    Returns:\n",
    "      results: dict with:\n",
    "        - \"cv_f1_scores\"\n",
    "        - \"cv_f1_mean\"\n",
    "        - \"cv_f1_std\"\n",
    "        - \"test_pred\" (binary 0/1)\n",
    "        - \"test_proba\" (or None if not available)\n",
    "        - \"train_time_sec\"\n",
    "    \"\"\"\n",
    "    splits = get_cv_splits(y, config)\n",
    "    threshold = config[\"DECISION_THRESHOLD\"]\n",
    "\n",
    "    fold_scores = []\n",
    "    start_time = time()\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(splits, start=1):\n",
    "        print(f\"\\n=== {model_name}: Fold {fold}/{len(splits)} ===\")\n",
    "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "        model = build_model_fn()\n",
    "\n",
    "        # Fit\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        # Predict on validation\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            val_proba = model.predict_proba(X_val)[:, 1]\n",
    "            val_pred = (val_proba >= threshold).astype(int)\n",
    "        else:\n",
    "            # fallback: direct class prediction\n",
    "            val_pred = model.predict(X_val)\n",
    "\n",
    "        f1 = f1_score(y_val, val_pred, average=\"macro\")\n",
    "        print(f\"Fold {fold} F1 Macro: {f1:.4f}\")\n",
    "        fold_scores.append(f1)\n",
    "\n",
    "    train_time_sec = time() - start_time\n",
    "\n",
    "    cv_mean = float(np.mean(fold_scores))\n",
    "    cv_std = float(np.std(fold_scores))\n",
    "    print(\"\\n=== CV Summary ===\")\n",
    "    print(f\"{model_name}: F1 Macro mean = {cv_mean:.4f}, std = {cv_std:.4f}\")\n",
    "    print(f\"Training + CV time: {train_time_sec:.1f} sec\")\n",
    "\n",
    "    # Fit on full training data and predict on test\n",
    "    full_model = build_model_fn()\n",
    "    full_model.fit(X, y)\n",
    "\n",
    "    test_proba = None\n",
    "    if hasattr(full_model, \"predict_proba\"):\n",
    "        test_proba = full_model.predict_proba(X_test)[:, 1]\n",
    "        test_pred = (test_proba >= threshold).astype(int)\n",
    "    else:\n",
    "        test_pred = full_model.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        \"cv_f1_scores\": fold_scores,\n",
    "        \"cv_f1_mean\": cv_mean,\n",
    "        \"cv_f1_std\": cv_std,\n",
    "        \"test_pred\": test_pred.astype(int),\n",
    "        \"test_proba\": test_proba,\n",
    "        \"train_time_sec\": train_time_sec,\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ea428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 6: Submission helper\n",
    "# =========================\n",
    "\n",
    "def save_submission(test_ids, test_pred, config=CONFIG, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    test_ids: list of ids from test.json (must match Kaggle id column)\n",
    "    test_pred: numpy array of 0/1 predictions\n",
    "    suffix: e.g. \"lgbm_v1\", \"xgb_pca256\", etc.\n",
    "    \"\"\"\n",
    "    df_sub = pd.DataFrame({\n",
    "        \"row_id\": test_ids,       # Kaggle expects row_id as per manual\n",
    "        \"target\": test_pred.astype(int),\n",
    "    })\n",
    "    out_dir = Path(config[\"SUBMISSION_OUTPUT_DIR\"])\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    exp_name = config[\"EXP_NAME\"]\n",
    "    if suffix:\n",
    "        filename = f\"{exp_name}_{suffix}.csv\"\n",
    "    else:\n",
    "        filename = f\"{exp_name}.csv\"\n",
    "\n",
    "    out_path = out_dir / filename\n",
    "    df_sub.to_csv(out_path, index=False)\n",
    "    print(f\"Saved submission to: {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbbbd5",
   "metadata": {},
   "source": [
    "## train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 9: MLP (early fusion) using sklearn\n",
    "# =========================\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def build_mlp_early_model(config=CONFIG):\n",
    "    \"\"\"\n",
    "    MLPClassifier params:\n",
    "      - hidden_layer_sizes: tuple of layer sizes, e.g. (512, 256)\n",
    "      - activation: \"relu\" | \"tanh\" | \"logistic\" | \"identity\"\n",
    "      - solver: \"adam\" (good default) | \"lbfgs\" | \"sgd\"\n",
    "      - alpha: L2 regularization term (1e-5–1e-3 typical)\n",
    "      - batch_size: \"auto\" or int\n",
    "      - learning_rate_init: initial LR (1e-4–1e-2)\n",
    "      - max_iter: epochs/iterations; set high (100–500+) and rely on early_stopping\n",
    "      - early_stopping: True/False (splits off validation part internally)\n",
    "    NOTE:\n",
    "      - MLP is sensitive to scaling; SCALER_TYPE should NOT be \"none\".\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_layer_sizes = (512, 256)  # e.g. (512,), (512, 256), (256, 128, 64)\n",
    "\n",
    "    params = {\n",
    "        \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "        \"activation\": \"relu\",          # \"relu\" | \"tanh\" | \"logistic\" | \"identity\"\n",
    "        \"solver\": \"adam\",              # \"adam\" good default\n",
    "        \"alpha\": 1e-4,                 # L2 penalty; try 1e-5–1e-3\n",
    "        \"batch_size\": \"auto\",          # or int like 128, 256\n",
    "        \"learning_rate_init\": 1e-3,    # base LR; try 1e-4–1e-2\n",
    "        \"max_iter\": 200,               # max epochs/iterations\n",
    "        \"early_stopping\": True,        # uses internal val split\n",
    "        \"n_iter_no_change\": 10,        # patience for early stopping\n",
    "        \"random_state\": config[\"SEED\"],\n",
    "    }\n",
    "\n",
    "    # Class imbalance\n",
    "    if config[\"IMBALANCE_MODE\"] == \"balanced\":\n",
    "        params[\"class_weight\"] = \"balanced\"\n",
    "\n",
    "    model = MLPClassifier(**params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86311c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_results = run_cv_and_fit_full(build_mlp_early_model, X_train_proc, y_train, X_test_proc, CONFIG, model_name=\"MLP_Early\")\n",
    "sub_path = save_submission(test_ids, mlp_results[\"test_pred\"], CONFIG, suffix=\"mlp_early_v1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
